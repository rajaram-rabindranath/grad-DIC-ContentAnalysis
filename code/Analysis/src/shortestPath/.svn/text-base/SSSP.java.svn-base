package shortestPath;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counters;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class SSSP {

	private static final transient Logger LOG = LoggerFactory.getLogger(SSSP.class);

	private static Properties properties = new Properties();

	static enum MoreIterations {
		numberOfIterations
	}

	public static class SearchMapperSSSP extends SearchMapper {

		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {

			Node inNode = new Node(value.toString());
			super.map(key, value, context, inNode);

		}
	}

	public static class SearchReducerSSSP extends SearchReducer {

		public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

			Node outNode = new Node();

			outNode = super.reduce(key, values, context, outNode);

			if (outNode.getColor() == Node.Color.GRAY)
				context.getCounter(MoreIterations.numberOfIterations).increment(1L);

		}
	}

	public static void main(String[] args) throws Exception {

		Configuration conf = new Configuration();
		LOG.info("HDFS Root Path: {}", conf.get("fs.defaultFS"));
		LOG.info("MR Framework: {}", conf.get("mapreduce.framework.name"));
		properties.load(SSSP.class.getResourceAsStream("/conf/configuration.properties"));
		LOG.info(properties.getProperty("output_path"));

		int tasks = 1;
		try {
			tasks = Integer.parseInt(args[0]);
		} catch (NumberFormatException e) {

		} catch (ArrayIndexOutOfBoundsException e) {

		}

		deleteFolder(conf, properties.getProperty("output_path"));

		int iterationCount = 0;

		long terminationValue = 1;

		while (terminationValue > 0) {

			Job job = Job.getInstance(conf);
			cacheHeaders(job);
			job.setNumReduceTasks(tasks);
			job.setJarByClass(SSSP.class);
			job.setMapperClass(SearchMapperSSSP.class);
			job.setReducerClass(SearchReducerSSSP.class);
			job.setOutputKeyClass(Text.class);
			job.setOutputValueClass(Text.class);

			String input, output;

			if (iterationCount == 0) {
				input = args[0];
				deleteFolder(job.getConfiguration(), args[1] + (iterationCount + 1));
			} else
				input = args[1] + iterationCount;

			output = args[1] + (iterationCount + 1);

			FileInputFormat.setInputPaths(job, new Path(input));
			FileOutputFormat.setOutputPath(job, new Path(output));

			job.waitForCompletion(true);

			Counters jobCntrs = job.getCounters();
			terminationValue = jobCntrs.findCounter(MoreIterations.numberOfIterations).getValue();
			iterationCount++;

		}
	}

	private static void deleteFolder(Configuration conf, String folderPath) throws IOException {
		FileSystem fs = FileSystem.get(conf);
		Path path = new Path(folderPath);
		if (fs.exists(path)) {
			fs.delete(path, true);
		}
	}

	private static void cacheHeaders(Job job) throws IOException {
		Configuration conf = job.getConfiguration();
		FileSystem fs = FileSystem.get(conf);
		Path hdfsPath = new Path(properties.getProperty("data_out_path"));

		fs.copyFromLocalFile(false, true, new Path(properties.getProperty("data_in_path")), hdfsPath);

		try {
			job.addCacheFile(new URI(properties.getProperty("data_out_path")));
		} catch (URISyntaxException e) {
			e.printStackTrace();
		}
	}
}